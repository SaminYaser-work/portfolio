[["Map",1,2,9,10,45,46],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.1.9","content-config-digest","7176a42bda089588","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"server\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":false,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"@astrojs/vercel/dev-image-service\",\"config\":{\"sizes\":[640,750,828,1080,1200,1920,2048,3840],\"domains\":[],\"remotePatterns\":[]}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"shiki\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,28,29],"generating-word-cloud-from-bengali-text-using-python",{"id":11,"data":13,"body":21,"filePath":22,"assetImports":23,"digest":25,"legacyId":26,"deferredRender":27},{"title":14,"tags":15,"image":17,"description":18,"date":19,"draft":20},"Generating Word Cloud from Bengali Text using Python",[16],"python","__ASTRO_IMAGE_/src/assets/img/blog/wordcloud/wc1.png","",["Date","2023-09-29T18:00:00.000Z"],false,"import { Image } from \"astro:assets\";\r\nimport ImageWithCaption from \"../../components/ImageWithCaption.astro\";\r\nimport PostMetadata from \"../../components/PostMetadata.astro\";\r\nimport wc1 from \"../../assets/img/blog/wordcloud/wc1.png\";\r\nimport wc2 from \"../../assets/img/blog/wordcloud/wc2.png\";\r\nimport wc3 from \"../../assets/img/blog/wordcloud/wc3.png\";\r\nimport bd from \"../../assets/img/blog/wordcloud/bd.jpg\";\r\nimport Blockquote from \"../../components/Blockquote.astro\";\r\n\r\n# {props.data.title}\r\n\r\n\u003CPostMetadata\r\n  time={props.time}\r\n  date={props.date}\r\n  description={props.data.description}\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={wc1}\r\n  alt=\"Bengali wordcloud\"\r\n  caption=\"Bengali wordcloud\"\r\n/>\r\n\r\n## Introduction\r\nWord cloud is a visual representation of a set of words. It is a useful tool for understanding the frequency of words in a body of text. Word clouds are often used to describe the contents of a dataset in various **NLP** related projects.\r\n\r\nIn this tutorial, we will generate a word cloud from a Bengali dataset using `wordcloud` library in python. We will be working with a dataset consists of Wikipedia articles, most commonly known as the **Wikipedia Dataset**. It can be downloaded from [here](https://huggingface.co/datasets/wikipedia). You can also get it from [here](https://dumps.wikimedia.org/), but in this case you have to manually **extract and clean** the articles from the XML file. Maybe I'll write a tutorial on how to do that in the future.\r\n\r\nNote that it is a big dataset, so the code in this tutorial is optimized for memory.\r\n\r\n## Importing Libraries\r\nFirst, we import the necessary libraries.\r\n\r\n```python\r\nimport re\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom wordcloud import WordCloud, ImageColorGenerator\r\nimport matplotlib.pyplot as plt\r\nfrom collections import Counter\r\n```\r\n`re`, `wordcloud` and `matplotlib` are the libraries we need to generate and display the word cloud.\r\n\r\n`numpy` and `PIL` are needed if you want generate the word cloud in a specific shape, so it can be ommitted if your are not interested in that.\r\n\r\n`Counter` is recommended if you have a large dataset and low on RAM.\r\n\r\n## Removing Stop Words\r\nStop words are words which are not important to the meaning of the text. They are usually used most frequently in the text. So, generating a wordcloud with stop words will not give us the most meaningful result. Look at the figure below and see for yourself.\r\n\r\n\r\n\u003CImageWithCaption\r\n  src={wc3}\r\n  alt=\"Bengali wordcloud with stop words\"\r\n  caption=\"Bengali wordcloud with stop words\"\r\n/>\r\n\r\nEasiest way to remove stop words are to use [bnlp-toolkit](https://pypi.org/project/bnlp-toolkit/) library. This is how you do it.\r\n\r\n```python\r\n# Install bnlp-toolkit with this command\r\n# pip install bnlp-toolkit\r\n\r\nfrom bnlp.corpus import stopwords\r\nfrom bnlp.corpus.util import remove_stopwords\r\n\r\nraw_text = 'à¦†à¦®à¦¿ à¦­à¦¾à¦¤ à¦–à¦¾à¦‡à¥¤'\r\nresult = remove_stopwords(raw_text, stopwords)\r\nprint(result)\r\n# ['à¦­à¦¾à¦¤', 'à¦–à¦¾à¦‡', 'à¥¤']\r\n```\r\n\r\nPretty easy, right? However, if you have your own list of stop-words, you can do this to clean your dataset.\r\n\r\n```python\r\nwith open('../Datasets/stopwords.txt', 'r', encoding='utf-8') as fin:\r\n  stopwords = fin.read().splitlines()\r\n\r\nwith open('../Datasets/wiki/all_v2_clean_sentence_no_stopwords.txt', 'w', encoding='utf-8') as fout:\r\n  with open('../Datasets/wiki/all_v2_clean_sentence.txt', 'r', encoding='utf-8') as fin:\r\n    for line in tqdm(fin):\r\n      words = line.split(' ')\r\n      clean_line = [word for word in words if word not in stopwords]\r\n      clean_line = ' '.join(clean_line)\r\n      fout.write(clean_line)\r\n```\r\nHere, we are cleaning the dataset from stop words *line by line*, which is a bit slow, but saves memory if you have a large dataset. Feel free to do it all at once if your dataset is small.\r\n\r\nNote that you are also saving the cleaned dataset in a new text file so that we don't have to repeat this time consuming process ever again.\r\n\r\n## Getting the most frequent words\r\nThis step is optional if you have a small dataset. But if you have a large dataset, `wordcloud` function might eat up all your memory and crash your PC. To stop this, we use `Counter` from `collections` library, which can count the frequency of each word in a dataset in a memory friendly way. We can then use these frequently used words to generate the word cloud. In my experience, this method is also faster than just simply passing the whole dataset to generate the word cloud.\r\n\r\n```python\r\nwith open('../Datasets/wiki/all_v2_clean_sentence_no_stopwords.txt', 'r', encoding='utf-8') as fin:\r\n  text = fin.read().split(' ')\r\n\r\ncounter = Counter(text)\r\n\r\nmost_common_words = counter.most_common(2000) # Use 2000 most common words to generate the word cloud\r\n```\r\n\r\n## Generating the Word Cloud â˜ï¸\r\nFinally, we can generate the word cloud. We are also giving it the shape of Bangladeshi flag.\r\n\r\n\u003CImageWithCaption\r\n  src={bd}\r\n  alt=\"Flag of Bangladesh\"\r\n  caption=\"Flag of Bangladesh\"\r\n/>\r\n\r\nI also suggest getting a Bengali font. I am using [Nirmala UI](https://www.wfonts.com/font/nirmala-ui), which is the default Bengali font in _Windows_.\r\n\r\n```python\r\nregex = r\"([\\S]+)\"\r\nmask = np.array(Image.open(\"bd.jpg\")) # Load your picture here\r\n\r\ndef plot_world_cloud(text):\r\n\r\n    wordcloud = WordCloud(\r\n        # Width and height of the word cloud image\r\n        width = 1000,\r\n        height = 500,\r\n\r\n        mode='RGBA',\r\n        background_color ='white',\r\n        font_path='fonts/nirmala.ttf', # Use your own font here\r\n        regexp=regex,\r\n        collocations=True,\r\n        mask=mask,\r\n\r\n        # Maximum number of words in the word cloud\r\n        max_words=2000,\r\n    )\r\n\r\n    wordcloud.generate(text)\r\n\r\n    # plot the WordCloud image\r\n    image_colors = ImageColorGenerator(mask)\r\n    plt.figure(figsize = (15, 15))\r\n    plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\r\n    plt.axis(\"off\")\r\n    plt.tight_layout(pad = 0)\r\n\r\n    plt.show()\r\n\r\nplot_world_cloud(' '.join([i[0] for i in most_common_words]))\r\n```\r\n\r\n\u003CImageWithCaption\r\n  src={wc3}\r\n  caption=\"Word Cloud with Custom Shape\"\r\n/>\r\n\r\n\u003CBlockquote type=\"warning\">\r\n  Passing `regex = r\"&#40;[\\S]+&#41;\"` to the `wordcloud` function is **crucial**. Otherwise, your output image might look something like this where conjunctive letters are broken.\r\n\u003C/Blockquote>\r\n\r\n\r\n\u003CImageWithCaption\r\n  src={wc2}\r\n  alt=\"Broken Conjunctive Letters\"\r\n  caption=\"Broken Conjunctive Letters\"\r\n/>\r\n\r\n## Alternatives for Word Cloud\r\nYou might also want check out [this](https://www.kaggle.com/code/paultimothymooney/most-common-words-on-kaggle-wordcloud-bargraph/notebook) where a guy generated a bar chart out of the most common words in his dataset. This may not be as good looking as a word cloud, but if you want go a more statistical route, this is the way to go.\r\n\r\n## Conclusion\r\nThis is a good starting point for generating word clouds. Hope it helped you. Heres a Github [gist](https://gist.github.com/SaminYaser-work/11ea93a474b2da7e843b83dccaf1bbcc) with the code.","src/content/blog/Generating Word Cloud from Bengali Text using Python.mdx",[24],"/src/assets/img/blog/wordcloud/wc1.png","1b9ebcef47f8498c","Generating Word Cloud from Bengali Text using Python.mdx",true,"typesafe-wordpress-block-attributes",{"id":28,"data":30,"body":39,"filePath":40,"assetImports":41,"digest":43,"legacyId":44,"deferredRender":27},{"title":31,"tags":32,"image":36,"description":37,"date":38,"draft":20},"Type-safe Wordpress Block Attributes with TypeScript and JSDoc",[33,34,35],"wordpress","typescript","javascript","__ASTRO_IMAGE_/src/assets/img/blog/typesafe-attr/main.png","Working with WordPress block attributes can be a pain. TypeScript and JSDoc can help you with that. Here is how.",["Date","2024-02-23T00:00:00.000Z"],"import ImageWithCaption from '../../components/ImageWithCaption.astro'\r\nimport PostMetadata from '../../components/PostMetadata.astro'\r\nimport Blockquote from '../../components/Blockquote.astro'\r\n\r\n{/* Images */}\r\nimport mainImg from '../../assets/img/blog/typesafe-attr/main.png'\r\nimport error2 from '../../assets/img/blog/typesafe-attr/error2.png'\r\nimport autocmp1 from '../../assets/img/blog/typesafe-attr/autocmp1.png'\r\nimport autocmp2 from '../../assets/img/blog/typesafe-attr/autocmp2.png'\r\n\r\n\r\n# {props.data.title}\r\n\r\n\u003CPostMetadata time={props.time} date={props.date} description={props.data.description} />\r\n\r\n\u003CImageWithCaption src={mainImg} alt=\"block attributes with typescript\"  />\r\n\r\n## Introduction\r\n[Block attributes](https://developer.wordpress.org/block-editor/reference-guides/block-api/block-attributes/) in WordPress are used to define and control the behavior of a block. They are essentially properties that you can use to customize a block and its content. Attributes can be used to store and control various aspects of a block, such as: the content of the block, settings and configurations, styling options etc.\r\nAttributes are defined in the block's `block.json` or in the `registerBlockType` function in JavaScript, and they can be of various types, such as **strings, numbers, objects, arrays, and booleans**. They are automatically passed to the block's `edit` and `save` functions, and we can use them there as well as manipulate them.\r\n\r\n\u003Cbr/>\r\n\r\nOne of the _gripes_ I have with the block attributes is that they are not typed, at least for now. This means that you don't get any type checking or autocompletion when working with them. It is manageable if you have a few attributes, but as the number of attributes grows, it becomes a pain to manage them. You shouldn't have more than a few attributes per block anyway, but sometimes you have to. For example, I was working on building a block will manage popups and I had more than 50 attributes. If I had to manage the attributes manually, and it would be awful. I would have to keep track of the types and default values of each attribute, which would have been error-prone and time-consuming. In this article, I will show you a neat trick on how added type checking and autocompletion to block attributes  with **TypeScript** and **JSDoc**!\r\n\r\n## Setting up TypeScript\r\n\r\nI assume you have already setup a block using `create-block` package. If not, you can follow the [official documentation](https://developer.wordpress.org/block-editor/reference-guides/packages/packages-create-block/) to create a block. After you are done, you can install TypeScript in your project by running the following command.\r\n\r\n```shell\r\nnpm install --save-dev typescript\r\n```\r\n\r\nNext up, we shall configure the settings for TypeScript. Create a `tsconfig.json` file in the root of your project and paste in the following configuration.\r\n\r\n```json\r\n{\r\n\t\"compilerOptions\": {\r\n\t\t\"allowJs\": true,\r\n\t\t\"checkJs\": true,\r\n\t\t\"strict\": true,\r\n\t\t\"jsx\": \"react-jsx\",\r\n\t\t\"resolveJsonModule\": true,\r\n\t\t\"esModuleInterop\": true,\r\n\t\t\"noEmit\": true\r\n\t},\r\n\t\"include\": [\"./src/**/*\"],\r\n}\r\n\r\n```\r\n\r\nHere's a short explanation on what these options do. \r\n\r\n- `allowJs`, `checkJs` and `strict` allows JavaScript files to be compiled and type-checked. \r\n\r\n- `\"jsx\": \"react-jsx\"` specifies the JSX factory to use for JSX syntax. \r\n\r\n- `\"resolveJsonModule\": true` Allows importing of JSON modules from TypeScript files. Why do we need this? We will see later ðŸ¤«\r\n\r\n- `\"esModuleInterop\": true` Enables a more compatible CommonJS/AMD module emit and finally,\r\n\r\n- `\"noEmit\": true` stops the compiler from emitting output files like JavaScript source code or declaration files because we only want to check the types.\r\n\r\n\u003CBlockquote type=\"info\">\r\n\tYou can set `\"strict\"` to `false` in the `tsconfig.json` file if you are working with an existing project and want to gradually introduce TypeScript and not get overwhelmed by all the errors and warnings in your IDE.\r\n\u003C/Blockquote>\r\n\r\n\r\n## Defining Some Attributes\r\n\r\nLet's define some attributes for our block. Open the `block.json` file and add the following attributes.\r\n\r\n\r\n```json\r\n{\r\n\t\"attributes\": {\r\n\t\t\t\"title\": {\r\n\t\t\t\t\"type\": \"string\",\r\n\t\t\t\t\"default\": \"\"\r\n\t\t\t},\r\n\r\n\t\t\t\"content\": {\r\n\t\t\t\t\"type\": \"string\",\r\n\t\t\t\t\"default\": \"\"\r\n\t\t\t},\r\n\t\t\t\"color\": {\r\n\t\t\t\t\"type\": \"string\",\r\n\t\t\t\t\"default\": \"#ffffff\"\r\n\t\t\t},\r\n\r\n\t\t\t\"fontSize\": {\r\n\t\t\t\t\"type\": \"number\",\r\n\t\t\t\t\"default\": \"16\"\r\n\t\t\t}\r\n\t}\r\n}\r\n```\r\n\r\n## Typing the block attributes (Noob way ðŸ˜…)\r\n\r\nThe most obvious way to type the block attributes is to use TypeScript interfaces. Simply create a `types.ts` file in the `src` directory and define the types there for both the `attributes` object and the `setAttributes` function.\r\n\r\n```typescript\r\nexport type Attributes {\r\n\ttitle: string;\r\n\tcontent: string;\r\n\tcolor: string;\r\n\tfontSize: number;\r\n}\r\n\r\nexport type SetAttributesType = (attributes: Partial\u003CAttributes>) => void;\r\n```\r\n\r\n\u003Cbr/>\r\nThen, import the types with JSDoc in your JavaScript file.\r\n\r\n```javascript\r\n/**\r\n * @typedef {Object} Props\r\n * @property {import('./types').AttributeType} attributes\r\n * @property {import('./types').SetAttributesType} setAttributes\r\n */\r\n\r\n/**\r\n *\r\n * @param {Props} props\r\n * @returns {JSX.Element}\r\n */\r\nexport default function Edit({ attributes, setAttributes }) {\r\n  //...\r\n}\r\n\r\n\r\n```\r\n\r\n\u003Cbr/>\r\nDone! Enjoy your auto completions and type checking. ðŸŽ‰\r\n\r\n\u003CImageWithCaption src={autocmp1} caption=\"Auto-completions for Block Attributes\"  isMaxWidthFull={true} />\r\n\u003CImageWithCaption src={error2} caption=\"Warnings when you make a mistake\"  isMaxWidthFull={true} />\r\n\r\n## Typing the block attributes (Advanced ðŸ˜Ž)\r\n\r\nThe problem with the previous approach is that it requires you to _manually_ keep the TypeScript types in sync with the block attributes. A better approach is to generate the TypeScript types from the `block.json` file. This way, the TypeScript types will always be in sync with the block attributes. To do this, simply import the `block.json` file and infer the type from there. This is why we needed `\"resolveJsonModule\": true` in the `tsconfig.json` file ðŸ˜„\r\n\r\n```typescript\r\nimport { attributes } from \"../block.json\";\r\n\r\ntype BlockAttributesKeys = keyof typeof attributes;\r\n\r\nexport type AttributeType = {\r\n\t[K in BlockAttributesKeys]: \"enum\" extends keyof (typeof attributes)[K]\r\n\t\t? (typeof attributes)[K][\"enum\"] extends Array\u003Cinfer T>\r\n\t\t\t? T\r\n\t\t\t: never\r\n\t\t: \"default\" extends keyof (typeof attributes)[K]\r\n\t\t? (typeof attributes)[K][\"default\"]\r\n\t\t: \"type\" extends keyof (typeof attributes)[K]\r\n\t\t? (typeof attributes)[K][\"type\"]\r\n\t\t: any;\r\n};\r\n\r\nexport type SetAttributesType = (attributes: Partial\u003CAttributeType>) => void;\r\n\r\n```\r\n\r\n\u003Cbr/>\r\nIt can also pick up the default values from the `block.json` file. This way, you don't have to manually specify the default values in the TypeScript types. Like for example, I have fairly complex `containerBackground` attribute.\r\n\r\n```json\r\n{\r\n\t\"containerBorder\": {\r\n\t\t\"type\": \"object\",\r\n\t\t\"default\": {\r\n\t\t\t\"width\": {\r\n\t\t\t\t\"top\": 0,\r\n\t\t\t\t\"right\": 0,\r\n\t\t\t\t\"bottom\": 0,\r\n\t\t\t\t\"left\": 0,\r\n\t\t\t\t\"unit\": \"px\"\r\n\t\t\t},\r\n\t\t\t\"type\": \"solid\",\r\n\t\t\t\"color\": \"#555d66\",\r\n\t\t\t\"openBorder\": 0\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\u003Cbr/>\r\nAnd it gets typed perfectly!\r\n\r\n\u003CImageWithCaption src={autocmp2} caption=\"Auto-completion for Complex Attribute\"  isMaxWidthFull={true} />\r\n\r\n### Problem with Enum Types\r\nYou will notice that the `AttributeType` type has a problem with `enum` types. It doesn't pick up the enum values from the `block.json` file. For example: if you have an attribute like this in the `block.json` file:\r\n```json\r\n\t\"test1\": {\r\n\t\t\"enum\": [\"a\", \"b\", \"c\"]\r\n\t},\r\n```\r\n\r\n\u003Cbr/>\r\nThen, `test1` should be of type `\"a\" | \"b\" | \"c\"`, but it's not. It't just `string`. This is because TypeScript doesn't support inferring string literal types from JSON files. There is no way other than manually specifying the enum types in the TypeScript types. This is a bit of a bummer, but it's not a big deal if you only have a few enum types. Here's how you can do it and this is the _**final version**_ of the `types.ts` file. I have also added a few helper types to make the code more readable.\r\n\r\n```ts\r\nimport { attributes } from \"../block.json\";\r\n\r\n// Define your enum types here\r\ntype Test1EnumType = 1 | \"b\" | \"c\";\r\ntype Test2EnumType = \"a\" | \"b\" | \"c\";\r\n\r\ntype EnumMapping = {\r\n\ttest1: Test1EnumType;\r\n\ttest2: Test2EnumType;\r\n};\r\n\r\n// Helper types\r\ntype GetValueTypeFromKey\u003CT, K> = K extends keyof typeof attributes\r\n\t? T extends keyof (typeof attributes)[K]\r\n\t\t? (typeof attributes)[K][T]\r\n\t\t: never\r\n\t: never;\r\n\r\ntype DoesKeyExist\u003CT, K> = K extends keyof typeof attributes\r\n\t? T extends keyof (typeof attributes)[K]\r\n\t\t? true\r\n\t\t: false\r\n\t: false;\r\n\r\nexport type AttributeType = {\r\n\t[K in keyof typeof attributes]: K extends keyof EnumMapping\r\n\t\t? EnumMapping[K]\r\n\t\t: DoesKeyExist\u003C\"default\", K> extends true\r\n\t\t? GetValueTypeFromKey\u003C\"default\", K>\r\n\t\t: DoesKeyExist\u003C\"type\", K> extends true\r\n\t\t? GetValueTypeFromKey\u003C\"type\", K>\r\n\t\t: any;\r\n};\r\n\r\nexport type SetAttributesType = (attributes: Partial\u003CAttributeType>) => void;\r\n```\r\n\r\n```js\r\n// Importing and applying the types in the JavaScript file\r\n/**\r\n * @typedef {Object} Props\r\n * @property {import('./types').AttributeType} attributes\r\n * @property {import('./types').SetAttributesType} setAttributes\r\n */\r\n\r\n/**\r\n *\r\n * @param {Props} props\r\n * @returns {JSX.Element}\r\n */\r\nexport default function Edit({ attributes, setAttributes }) {\r\n  //...\r\n}\r\n```\r\n\r\n## Conclusion\r\n\r\nThat's it! You now have type checking and auto-completion for your block attributes. This will make your life easier and your code more robust. You can now easily manage and manipulate your block attributes without worrying about making mistakes. I hope you found this article helpful. Happy coding! ðŸš€","src/content/blog/Typesafe Wordpress Block Attributes.mdx",[42],"/src/assets/img/blog/typesafe-attr/main.png","6138b7fd2c830303","Typesafe Wordpress Block Attributes.mdx","project",["Map",47,48,64,65,83,84,102,103,118,119,135,136,151,152],"autoturret",{"id":47,"data":49,"body":58,"filePath":59,"assetImports":60,"digest":62,"legacyId":63,"deferredRender":27},{"title":50,"tags":51,"image":54,"description":55,"date":56,"ghUrl":57,"draft":20,"featured":20},"AutoTurret - Arduino controlled Laser Turret with Computer Vision",[52,53],"arduino","computer vision","__ASTRO_IMAGE_/src/assets/img/project/autoturret/a1.jpeg","A turret that automatically detects and shoots targets",["Date","2023-10-01T00:00:00.000Z"],"https://github.com/SaminYaser-work/AutoTurret","import PostMetadata from \"../../components/PostMetadata.astro\";\r\nimport ImageWithCaption from \"../../components/ImageWithCaption.astro\";\r\nimport Latex from \"../../components/Latex.astro\";\r\nimport a1 from \"../../assets/img/project/autoturret/a1.jpeg\";\r\nimport f1 from \"../../assets/img/project/autoturret/f1.png\";\r\nimport f2 from \"../../assets/img/project/autoturret/f2.png\";\r\nimport f3 from \"../../assets/img/project/autoturret/f3.png\";\r\nimport real1 from \"../../assets/img/project/autoturret/real1.png\";\r\nimport real2 from \"../../assets/img/project/autoturret/real2.png\";\r\nimport m1 from \"../../assets/img/project/autoturret/1.png\";\r\nimport m2 from \"../../assets/img/project/autoturret/2.png\";\r\n\r\n# {props.data.title}\r\n\r\n\u003CPostMetadata\r\n  time={props.time}\r\n  date={props.date}\r\n  description={props.data.description}\r\n  ghUrl={props.data.ghUrl}\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={a1}\r\n  caption=\"Arduino Powered Auto-Turret\"\r\n/>\r\n\r\n## Project Overview\r\nThe security of individuals and communities has become increasingly important in recent times. This project\r\naimed to design and build an automated laser turret to enhance security measures in various settings, using\r\ncomputer vision and an Arduino microcontroller. The laser turret is capable of detecting human poses and aiming\r\nat potential threats, while also incorporating a \"surrender\" mode to prevent unnecessary harm. The project was\r\ndesigned with the belief that security is of paramount importance in modern society, and that technological\r\nadvancements can help to improve safety measures. The laser turret was designed using servos and lasers, and the\r\ncomputer vision aspect was achieved through the implementation of OpenCV software. The project was tested\r\nand refined to improve accuracy and speed, resulting in a successful final product. The survey conducted as part\r\nof this project aimed to gather information on the most important features of a laser turret, its potential uses, and\r\nthe public perception of such a technology. The survey results indicated that accuracy, cost, and range were the\r\nmost important factors, while the legality and privacy concerns of the technology were also deemed significant.\r\nOverall, this project presents a viable solution for security purposes, incorporating advanced technology while\r\nalso considering societal and cultural factors. The laser turret has the potential to enhance security measures in\r\nvarious settings, including election centers, public areas, and other locations where security is a concern.\r\n\r\n## Features\r\nThe automated turret system is designed to detect and track human targets using computer vision and laser diode\r\ntechnology. The system operates using the following step-by-step working principle:\r\n- **Human detection**: The system uses computer vision algorithms to detect and locate human targets within\r\nits field of view. This is achieved by analyzing the video stream from a camera mounted on the turret\r\nsystem.\r\n- **Human pose analysis**: The system performs human pose analysis to determine whether the detected\r\nhuman is in a surrender pose or not. This is done to avoid unnecessary use of force and ensure the safety\r\nof non-threatening individuals.\r\n- **Target tracking**: Once a human target is detected and identified as a potential threat, the system tracks\r\nthe target's movements using servo motors to adjust the position of the camera and laser diode.\r\n- **Laser aiming**: The laser diode is used to aim at the detected human target. This is done by adjusting the\r\nposition of the laser using the servo motors to ensure that the laser is pointing directly at the target.\r\n- **Warning countdown**: The system issues a warning countdown to the detected human target, giving them\r\nan opportunity to surrender and avoid the use of force.\r\n- **Shooting**: If the detected human target does not comply with the warning countdown, the system can be\r\nprogrammed to automatically shoot the laser diode at the target.\r\n\r\nThe flowchart below describes the overall process of the auto-targeting turret. The pre-defined\r\nprocesses in the flowchart are discussed in detail in the upcoming sections. In general, the automated turret system\r\nuses computer vision and laser diode technologies to detect, track, and possibly eliminate human threats. The use\r\nof servo motors to modify the position of the camera and laser diode in the system allows for precision aiming,\r\nwhile the warning countdown function allows the system to avoid the application of superfluous force\r\n\r\n\u003CImageWithCaption\r\n  src={f1}\r\n  caption=\"Flowchart of the Auto-Turret System\"\r\n/>\r\n\r\n## Auto-Aiming\r\nThe turret receives video feed from a stationary camera. Each frame of the video feed are analyzed to detect\r\nintruders and move the servo accordingly so that the lasers can be aimed at the target. This process is visualized\r\nin Figure 4. But a single frame is in pixel values. To calculate the servo degrees from the pixel values, we first\r\nneed to determine the range of the servo motor or the Field-of-View (FOV). The field of view of the camera is the\r\narea that the camera can see. This is determined by the camera lens and sensor size. Once we know the field of\r\nview, we can calculate the number of pixels per degree of rotation. This value will be unique for each camera and\r\nlens combination. In this experiment, we figured it out by manually moving the servos in a controlled manner.\r\n\r\n\u003CImageWithCaption\r\n  src={f2}\r\n  caption=\"Pixel to Degree Conversion\"\r\n/>\r\n\r\nThen, when the camera detects a human target, it calculates the position of the target within the image in terms of\r\nthe pixel values. These pixel values are then converted into degrees of rotation for the servo motor using the\r\npreviously calculated pixels per degree value. From the figure above, we get the following equations.\r\n\r\nexport const eq1 = '\\\\tan(\\\\alpha) = \\\\frac{a}{d} \\\\implies d = \\\\frac{a}{\\\\tan(\\\\alpha)}'\r\nexport const eq2 = '\\\\tan(\\\\alpha) = \\\\frac{b}{d} \\\\implies d = \\\\frac{b}{\\\\tan(\\\\alpha)}'\r\n\r\n\u003CLatex formula={eq1}/>\r\n\u003CLatex formula={eq2}/>\r\n\r\nSince, â€œdâ€ is common, we can get the value of Î² by the following equations:\r\n\r\nexport const eq3 = '\\\\tan(\\\\beta) = \\\\frac{b \\\\times \\\\tan(\\\\alpha)}{a}'\r\nexport const eq4 = '\\\\therefore \\\\beta = \\\\tan^{-1}(\\\\frac{b \\\\times \\\\tan(\\\\alpha)}{a})'\r\n\r\n\u003CLatex formula={eq3}/>\r\n\u003CLatex formula={eq4}/>\r\n\r\nThis Î² is the number of degrees the servos need to move to aim at the target. This calculated degree is fed to the\r\nservo motors so that it can accurately point the lasers at the target. In similar fashion, vertical movement is also\r\ncalculated. These two calculations are done for every frame, giving the turret the ability to automatically aim at\r\nany intruder visible in the camera feed.\r\n\r\n## Measuring Distance & Firing Effect\r\nTo measure the distance to a human target, we use a method called shoulder width calculation. This\r\ninvolves measuring the width of the target's shoulders in pixels within the camera image, and using this\r\nmeasurement to estimate their distance from the turret.\r\nTo begin, we use the landmark detection method described earlier to identify the target's shoulders within the\r\ncamera image. Once we have identified the shoulder landmarks, we calculate the distance between them in pixels.\r\nNext, we use the known width of an average human shoulder to estimate the actual distance to the target. This is\r\nbased on the assumption that the target's shoulder width will be proportional to their actual distance from the\r\nturret.\r\nBy continuously measuring the target's shoulder width as they move within the camera's field of view, we can\r\ntrack their distance and adjust the aim of the turret accordingly. This allows the system to accurately target human\r\nsubjects at a distance and respond appropriately to their movements.\r\nAlthough the turret does not actually fire bullets, we can simulate the shooting of a gun by alternating the laser\r\nlight and playing sound effects.\r\n\r\n\u003CImageWithCaption\r\n  src={f3}\r\n  caption=\"Misc. Flowcharts\"\r\n/>\r\n\r\n## Final Product\r\n\r\n\u003CImageWithCaption\r\n  src={real1}\r\n  caption=\"Final Product (Front)\"\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={real2}\r\n  caption=\"Final Product (Back)\"\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={m1}\r\n  caption=\"Tracking a Human Target 1\"\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={m2}\r\n  caption=\"Detecting Pose\"\r\n/>\r\n\r\n## Cost\r\n| Item                           | Quantity | Cost (BDT) |\r\n|--------------------------------|----------|------------|\r\n| Arduino Uno                    | 1        | 950        |\r\n| Camera mount with Servo Motors | 2        | 850        |\r\n| Laser Diode                    | 2        | 52         |\r\n| Jumper Wires                   | 1        | 150        |\r\n| Breadboard                     | 1        | 100        |\r\n| **Total**                      |          | 2102       |\r\n\r\n## Conclusion\r\nI had a lot of fun working on this project. Partly because I got to work with something tangible, rather than some pixel on\r\na screen. I also learned a lot about computer vision and how to use it in real life applications.","src/content/project/autoturret.mdx",[61],"/src/assets/img/project/autoturret/a1.jpeg","9c9eda295c9ddfd9","autoturret.mdx","agrosmart",{"id":64,"data":66,"body":77,"filePath":78,"assetImports":79,"digest":81,"legacyId":82,"deferredRender":27},{"title":67,"tags":68,"image":72,"description":73,"date":74,"ghUrl":75,"liveUrl":76,"draft":20,"featured":27},"AgroSmart - Full-fledged ERP for Agro Industries",[69,70,71],"laravel","fastapi","cloud","__ASTRO_IMAGE_/src/assets/img/project/agrosmart/1.jpeg","AgroSmart is a tier 2, AI-powered, specialized ERP system for agricultural industries. Manage agricultural, aquaculture and livestock-related operations with ease.",["Date","2023-10-04T00:00:00.000Z"],"https://github.com/SaminYaser-work/AgroSmart","https://agrosmart.azurewebsites.net/","import Blockquote from \"../../components/Blockquote.astro\";\r\nimport PostMetadata from \"../../components/PostMetadata.astro\";\r\nimport ImageWithCaption from \"../../components/ImageWithCaption.astro\";\r\nimport p1 from \"../../assets/img/project/agrosmart/1.jpeg\";\r\nimport dd from \"../../assets/img/project/agrosmart/dd.jpeg\";\r\nimport fish from \"../../assets/img/project/agrosmart/fish.jpeg\";\r\nimport sysArch from \"../../assets/img/project/agrosmart/sys-arch.png\";\r\nimport techstack from \"../../assets/img/project/agrosmart/techstack.png\";\r\n\r\n# {props.data.title}\r\n\r\n\u003CPostMetadata\r\n  time={props.time}\r\n  date={props.date}\r\n  description={props.data.description}\r\n  ghUrl={props.data.ghUrl}\r\n  liveUrl={props.data.liveUrl}\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={p1}\r\n  caption=\"AgroSmart Dashboard\"\r\n/>\r\n\r\n\u003CBlockquote type=\"info\">\r\n  The live version of this project is hosted on Azure on basic plan. It may take a few seconds to load. I'll keep it\r\n  live as long as my free credits last.\r\n\r\n  Login credentials: admin@mail.com\r\n  password: password\r\n\u003C/Blockquote>\r\n\r\n## Project Overview\r\nIn a dynamic and ever-evolving world, where agriculture, aquaculture, and livestock industries stand at the forefront of sustenance and economic growth, effective management across diverse domains is essential. Enter AgroSmart, the groundbreaking tier 2 ERP system that harnesses the power of artificial intelligence (AI) to deliver specialized solutions for the agricultural sector.\r\n\r\nAgroSmart not only addresses the intricacies of agriculture, aquaculture, and livestock-related operations but also integrates Human Capital Management (HCM), Sales, Supply Chain, and Inventory Management seamlessly into its ecosystem. This multifaceted platform offers a holistic approach to managing your agricultural ventures, fostering efficiency, sustainability, and growth.\r\n\r\nIn this project, we embark on a journey to uncover the innovative capabilities of AgroSmart, exploring how it empowers you to revolutionize your agricultural operations. We will dive deep into the realm of AgroSmart, demonstrating how it transforms the way you manage your workforce, streamline sales processes, and gain precise control over supply and inventory. Prepare to witness a new era of agricultural management with AgroSmart, where efficiency, sustainability, and growth are not just buzzwords, but a tangible reality.\r\n\r\n## System Architecture\r\n\u003CImageWithCaption\r\n  src={sysArch}\r\n  caption=\"System Architecture of AgroSmart\"\r\n/>\r\n\r\n## Techstack\r\n\u003CImageWithCaption\r\n  src={techstack}\r\n  caption=\"Techstack of AgroSmart\"\r\n/>\r\n\r\n## AI Integration: Revolutionizing Agricultural Practices\r\n\r\nOne of the core strengths of AgroSmart lies in its seamless integration of artificial intelligence (AI) to revolutionize agricultural practices. Through the power of AI, AgroSmart pioneers two key capabilities that redefine the way you approach crop cultivation and aquaculture management:\r\n\r\n### 1. Predicting Crop Diseases from Pictures\r\nImagine a scenario where identifying crop diseases is no longer reliant on manual inspection, but a quick snapshot from your smartphone. AgroSmart leverages AI-driven image recognition algorithms to analyze images of crops and instantly diagnose diseases or abnormalities. By examining subtle patterns, discolorations, and other visual cues, AgroSmart can accurately identify a wide range of crop diseases.\r\n\r\n\u003CImageWithCaption\r\n  src={dd}\r\n  caption=\"Diagnosing Crop Diseases\"\r\n/>\r\n\r\nThis transformative feature offers several advantages:\r\n\r\n- **Early Detection:** By catching diseases at their nascent stage, AgroSmart helps you take proactive measures to prevent the spread of infections, minimizing crop losses.\r\n\r\n- **Data-Driven Decisions:** The AI-powered diagnosis provides valuable data for decision-making, allowing you to select appropriate treatments or interventions tailored to specific diseases.\r\n\r\n- **Reduced Dependency:** With AI at your disposal, you reduce the need for extensive manual labor and the expertise required for disease identification, making your agricultural operations more cost-effective and accessible.\r\n\r\n### 2. Predicting Suitable Fish Species for Ponds\r\nIn aquaculture, selecting the right fish species for a pond is a critical decision. AgroSmart employs AI to predict the most suitable fish species based on various vitals gathered from the pond environment. By analyzing factors such as water temperature, pH levels, oxygen content, and nutrient concentrations, AgroSmart offers tailored recommendations that optimize fish health and growth.\r\n\r\n\u003CImageWithCaption\r\n  src={fish}\r\n  caption=\"Predicting Suitable Fish Species for Ponds\"\r\n/>\r\n\r\nHere's why this feature is a game-changer:\r\n\r\n- **Enhanced Productivity:** By choosing fish species best suited to your pond's conditions, you maximize yield and profitability.\r\n\r\n- **Environmental Compatibility:** AgroSmart ensures that your aquaculture practices align with the ecological characteristics of your pond, promoting sustainability and minimizing environmental impact.\r\n\r\n- **Risk Mitigation:** AI-driven predictions help mitigate the risks associated with choosing incompatible fish species, reducing the likelihood of disease outbreaks and other complications.\r\n\r\nIncorporating AI into the heart of AgroSmart's capabilities not only streamlines your agricultural and aquacultural operations but also empowers you with data-driven insights that elevate your decision-making process. With AgroSmart, you're not just managing your agricultural ventures; you're cultivating a smarter, more sustainable future for agriculture and aquaculture.","src/content/project/agrosmart.mdx",[80],"/src/assets/img/project/agrosmart/1.jpeg","7387de7847f58a61","agrosmart.mdx","instaclone",{"id":83,"data":85,"body":96,"filePath":97,"assetImports":98,"digest":100,"legacyId":101,"deferredRender":27},{"title":86,"tags":87,"image":91,"description":92,"date":93,"ghUrl":94,"liveUrl":95,"draft":20,"featured":27},"Instaclone - Recreating Instagram with Firebase",[88,89,34,90],"nextjs","tailwindcss","firebase","__ASTRO_IMAGE_/src/assets/img/project/instaclone/i1.png","A clone of Instagram with authentication, all CRUD operations, and Image uploading and real-time updates with Firebase",["Date","2022-12-10T00:00:00.000Z"],"https://github.com/SaminYaser-work/instaclone","https://instaclone-samin.vercel.app/","import Blockquote from \"../../components/Blockquote.astro\";\r\nimport PostMetadata from \"../../components/PostMetadata.astro\";\r\nimport ImageWithCaption from \"../../components/ImageWithCaption.astro\";\r\nimport i1 from \"../../assets/img/project/instaclone/i1.png\";\r\nimport i2 from \"../../assets/img/project/instaclone/i2.png\";\r\nimport i3 from \"../../assets/img/project/instaclone/i3.png\";\r\n\r\n# {props.data.title}\r\n\r\n\u003CPostMetadata\r\n  time={props.time}\r\n  date={props.date}\r\n  description={props.data.description}\r\n  ghUrl={props.data.ghUrl}\r\n  liveUrl={props.data.liveUrl}\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={i1}\r\n  caption=\"Instaclone Login Page\"\r\n/>\r\n\r\n\u003CBlockquote type=\"info\">\r\n  The live version may not be accessible due to firebase project expiring.\r\n\u003C/Blockquote>\r\n\r\n## Introduction\r\nThe project aims to develop a web application that replicates the core features of Instagram, a popular social media platform. This Instagram clone will provide users with the ability to create accounts, authenticate themselves, perform CRUD (Create, Read, Update, Delete) operations on posts, and upload images. Additionally, the application will incorporate real-time updates using Firebase, enhancing the user experience with seamless and interactive content sharing.\r\n\r\n\u003CImageWithCaption\r\n  src={i2}\r\n  caption=\"Instaclone Home Page\"\r\n/>\r\n\r\n## Key Features\r\n- **User Registration & Login**: Users can sign up for an account by providing their email, username, and password. It has two step onboarding process where user first register with email and then set their username and profile picture. The application will validate the userâ€™s email and password to ensure that they meet the requirements. The application will also check if the username is already taken. If the userâ€™s credentials are valid, the application will create an account for the user and redirect them to the login page. The user can then log in using their credentials to access the platform.\r\n- **CRUD Operations**: Developed functionality for users to create, read, update, and delete posts, fostering user engagement and content management.\r\n- **Content**: Enabled users to upload images, attaching them to their posts, to share visual content with others, comment on others post etc.\r\n- **Real-time Updates**: Utilized Firebase's real-time database capabilities to provide instant updates and notifications for actions such as likes, comments, and new posts.\r\n- **Mobile-Friendly Interface**: Ensured that the application is responsive and provides an excellent user experience on mobile devices, including smartphones and tablets.\r\n\r\n\u003CImageWithCaption\r\n  src={i3}\r\n  caption=\"Fully Mobile Responsive\"\r\n/>","src/content/project/instaclone.mdx",[99],"/src/assets/img/project/instaclone/i1.png","38027c781f1fd09c","instaclone.mdx","newsflash",{"id":102,"data":104,"body":112,"filePath":113,"assetImports":114,"digest":116,"legacyId":117,"deferredRender":27},{"title":105,"tags":106,"image":108,"description":109,"date":110,"ghUrl":111,"draft":20,"featured":20},"NewsFlash - RSS Feed Aggregator",[107],"C#","__ASTRO_IMAGE_/src/assets/img/project/newsflash/main.png","A RSS feed aggregator built with C# Windows Forms",["Date","2022-08-12T00:00:00.000Z"],"https://github.com/SaminYaser-work/NewsFlash","import PostMetadata from \"../../components/PostMetadata.astro\";\r\nimport ImageWithCaption from \"../../components/ImageWithCaption.astro\";\r\nimport main from \"../../assets/img/project/newsflash/main.png\";\r\n\r\n# {props.data.title}\r\n\r\n\u003CPostMetadata\r\n  time={props.time}\r\n  date={props.date}\r\n  description={props.data.description}\r\n  ghUrl={props.data.ghUrl}\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={main}\r\n  caption=\"NewsFlash Main Window\"\r\n/>\r\n\r\n## Project Overview\r\nThe system will be made as a Windows Form Application with .NET Framework. The\r\nsystem consists of a collection of forms which passes necessary data to each other to work in\r\nharmony. System also fetched data from the internet periodically or at the will of the user. So\r\na stable internet connection is required to operate this application successfully. The proposed\r\nsystem will only work on Windows operating system.\r\nThe application will save necessary data for example, email, name, password in the\r\ndatabase as well as the preferences of the user like settings, custom news sources, favourite\r\nlinks etc.\r\nTo get the updated news, the system will utilize RSS technology. In detail, the\r\napplication will download the XML file that a news service provides and parse it properly to\r\nget the title, publication date, summary, associated images etc. and display it to the user via a\r\nGUI application in a meaningful and intuitive way.\r\n\r\n## Features\r\n**General Features**:\r\n- Read news from pre-added websites.\r\n- Sharing news on social media.\r\n- Highlight unread news.\r\n- Open the article in a browser.\r\n- Copy the article link to the clipboard.\r\n\r\n**Primary Features**:\r\nThese features are tied to specific user accounts.\r\n- Add or remove new RSS links.\r\n- Favorite article link to save it or read later.\r\n- Change and save settings.\r\n\r\n**Miscellaneous Features**:\r\n- Change the color theme of the application.\r\n- Slick and modern look of the app that is easy on the eyes, minimal and not distracting.\r\n\r\n\r\n## Demo\r\nHere is a video of the project in action:\r\n\r\n\u003Cdiv className={\"w-fit mx-auto\"}>\r\n  \u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/0HQHLu1FU2s?si=LuYNQkNVK7a5FIdC\"\r\n          title=\"YouTube video player\" frameborder=\"0\"\r\n          allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\r\n          allowfullscreen>\u003C/iframe>\r\n\u003C/div>","src/content/project/newsflash.mdx",[115],"/src/assets/img/project/newsflash/main.png","a2798a2e00a1b129","newsflash.mdx","opengl-adventures",{"id":118,"data":120,"body":129,"filePath":130,"assetImports":131,"digest":133,"legacyId":134,"deferredRender":27},{"title":121,"tags":122,"image":125,"description":126,"date":127,"ghUrl":128,"draft":20,"featured":20},"OpenGL Adventures - Endless Runner in OpenGL",[123,124],"c++","opengl","__ASTRO_IMAGE_/src/assets/img/project/opengl-adventures/g1.png","Chrome Dino ðŸ¦– like game made with OpenGL (freeglut) and C++",["Date","2023-06-01T00:00:00.000Z"],"https://github.com/SaminYaser-work/OpenGL-Adventures","import PostMetadata from \"../../components/PostMetadata.astro\";\r\nimport ImageWithCaption from \"../../components/ImageWithCaption.astro\";\r\nimport g1 from \"../../assets/img/project/opengl-adventures/g1.png\";\r\n\r\n# {props.data.title}\r\n\r\n\u003CPostMetadata\r\n  time={props.time}\r\n  date={props.date}\r\n  description={props.data.description}\r\n  ghUrl={props.data.ghUrl}\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={g1}\r\n  caption=\"OpenGL Adventures\"\r\n/>\r\n\r\n## Project Overview\r\nSimple endless runner game made with C++ with openGL API and freeglut library. The absolute pain of working with C++ and\r\ndecade-old freeglut library was worth it. I learned a lot about OpenGL and C++ in the process. I wish I could make it look more\r\ngood... but we tried our best not to use any external library.\r\n\r\n## Challenges\r\nThe most challenging part of this project was to make the Synthwave-ish background. I had to use a lot of math and trigonometry\r\nto make it work. I also had to use a lot of math to make the player jump and fall in a realistic way. The discussed game is a 2D endless runner game. Due to this, the background scenery must\r\nscroll in a direction to give the impression that the player is moving forward. In addition, it must\r\nalso scroll infinitely until the game comes to an end. The major challenge faced when\r\nimplementing a scrolling background is ensuring the loop is seamless. It is important to hide the\r\nfact that one single image is being used to portray the background scenery. In this case, a seamless\r\nloop is essential to create an illusion that the background is scrolling infinitely. If it is the contrary,\r\nwhere the loop is not seamless and the player can spot the obvious joints, it would ruin the illusion\r\nand make apparent that the same image is being scrolled in a loop.\r\nTo seamlessly animate the stars, the field, and the road, the game updates the value of the\r\nX-axis (distance of the object from the left edge of the screen) of each individual object when\r\nrendering each frame, instead of creating new objects. This makes the game performance-friendly\r\nand reduces the stress on the CPU (instantiating new objects). The X-axis coordinate value resets\r\nwhen the object reaches the furthest left side of the image. To emulate depth perception or the\r\nparallax effect, these three things must move at different speeds. It generates the illusion that the\r\nroad is closer to the player than the field, and finally, the stars are the most distant. The sun is\r\nintentionally kept still since large celestial objects in the sky donâ€™t seem to move in the real world\r\nwhen viewing while moving.\r\nThe lines in the field are generated with a loop. For the horizontal lines, the distance\r\nbetween them decreases gradually. These types of lines are not animated. For the vertical lines,\r\nthey are initially backward leaning when on the right portion of the screen. When moving from\r\nright to left, the lines smoothly get straight in the middle and finally forward-leaning when on the\r\nleft side. This is done by X coordinates of the top-end of the line differently than the bottom-end\r\nof a line. With the combination of the vertical and the horizontal lines, it gives the impression of a\r\n3-dimensional open space, which is in fact, drawn with 2D shapes on a 2D plane.\r\n\r\n\u003Cbr/>\r\n\r\nPlease check out the [full report](https://www.academia.edu/78715394/OpenGL_Adventures?source=swp_share) for more details.","src/content/project/opengl-adventures.mdx",[132],"/src/assets/img/project/opengl-adventures/g1.png","9806c7c7a392e143","opengl-adventures.mdx","quran-cli",{"id":135,"data":137,"body":145,"filePath":146,"assetImports":147,"digest":149,"legacyId":150,"deferredRender":27},{"title":135,"tags":138,"image":141,"description":142,"date":143,"ghUrl":144,"draft":20,"featured":20},[139,140],"bash","linux","__ASTRO_IMAGE_/src/assets/img/project/quran-cli/main.png","Get the English translation of the verbatim word of Allah, the only God worthy of worship, right in your terminal.",["Date","2023-11-21T00:00:00.000Z"],"https://github.com/SaminYaser-work/quran-cli","import Blockquote from \"../../components/Blockquote.astro\";\r\nimport PostMetadata from \"../../components/PostMetadata.astro\";\r\nimport ImageWithCaption from \"../../components/ImageWithCaption.astro\";\r\nimport TableOfContents from \"../../components/TableOfContents.astro\";\r\nimport main from \"../../assets/img/project/quran-cli/main.png\";\r\nimport p1 from \"../../assets/img/project/quran-cli/1.png\";\r\n\r\n# {props.data.title}\r\n\r\n\u003CPostMetadata\r\n  time={props.time}\r\n  date={props.date}\r\n  description={props.data.description}\r\n  ghUrl={props.data.ghUrl}\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={main}\r\n  caption=\"quran-cli\"\r\n/>\r\n\r\n{/* \u003CTableOfContents headings={props.headings} /> */}\r\n\r\n\r\n## Project Overview\r\n\"quran-cli\" allows you to read the translation of Quran (with footnotes) in your terminal. It is written in pure bash and can be used by anyone who has a basic knowledge of the terminal and no internet is required! \r\n\r\n\u003CImageWithCaption\r\n  src={p1}\r\n  caption=\"quran-cli\"\r\n/>\r\n\r\n\u003Cbr/>\r\nFor now only the English translation by Saheeh International is available. I plan to add more translations in the future, insha'Allah.\r\n\r\n### Installation\r\n```bash\r\ncurl https://raw.githubusercontent.com/SaminYaser-work/quran-cli/master/install | sh\r\n```\r\n\r\n### Current Features\r\nFor the usage information, visit the \u003Ca target=\"_blank\" href=\"https://github.com/SaminYaser-work/quran-cli/blob/master/README.md\">Github Readme\u003C/a>.\r\n\r\n- Get full surah or a particular ayah by verse number, or a range of verses\r\n- Turn on/off footnotes, simple mode\r\n- Surah information\r\n- Bookmark ayahs\r\n\r\n### Developing\r\nUnit tests have been written for the project using BATS and shellcheck is used for linting. Instructions on installing BATS can be found \u003Ca href=\"https://bats-core.readthedocs.io/en/stable/tutorial.html#quick-installation\" target=\"_blank\">here\u003C/a>.\r\n\u003Cbr/>\r\nRun the unit tests:\r\n```bash\r\n./test/bats/bin/bats test/test.bats\r\n```","src/content/project/quran-cli.mdx",[148],"/src/assets/img/project/quran-cli/main.png","7c8f9fcd6e354a19","quran-cli.mdx","traffichain",{"id":151,"data":153,"body":162,"filePath":163,"assetImports":164,"digest":166,"legacyId":167,"deferredRender":27},{"title":154,"tags":155,"image":158,"description":159,"date":160,"ghUrl":161,"draft":20,"featured":27},"TraffiChain - Blockchain based Traffic Ticket Management System",[69,156,89,157],"react","blockchain","__ASTRO_IMAGE_/src/assets/img/project/traffichain/t1.png","Award Wining Smart Traffic Ticket Management System with Smart Contracts powered by Blockchain",["Date","2022-12-14T00:00:00.000Z"],"https://github.com/SaminYaser-work/TraffiChain","import PostMetadata from \"../../components/PostMetadata.astro\";\r\nimport ImageWithCaption from \"../../components/ImageWithCaption.astro\";\r\nimport t1 from \"../../assets/img/project/traffichain/t1.png\";\r\nimport t2 from \"../../assets/img/project/traffichain/t2.png\";\r\nimport t3 from \"../../assets/img/project/traffichain/t3.png\";\r\n\r\n# {props.data.title}\r\n\r\n\u003CPostMetadata\r\n  time={props.time}\r\n  date={props.date}\r\n  description={props.data.description}\r\n  ghUrl={props.data.ghUrl}\r\n/>\r\n\r\n\u003CImageWithCaption\r\n  src={t1}\r\n  caption=\"TraffiChain homepage\"\r\n/>\r\n\r\n## Introduction\r\nThe goal of this project is to introduce a transparent, corruption-free and trust-less method in issuing traffic tickets.\r\nA police officer can issue a traffic ticket which is represented as a smart contract.\r\nAuthorized parties like the driver and the court can interact with the contract to dismiss it by paying or fighting it.\r\nWhatever the choices are, the ticket can manage its own state.\r\nIt can keep track of deadlines, introduce late fees and can even cancel driver license if the need arises.\r\nImmutability of the blockchain ensures security and prevents man-in-the-middle attacks.\r\n\u003Cbr/>\r\n**This project was nominated to the Top 10 final round of CTO Innovation Hackathon 2022**\r\n\r\n\u003CImageWithCaption\r\n  src={t2}\r\n  caption=\"Registration page\"\r\n/>\r\n\r\n## How it Works\r\n* A database residing in blockchain contains information of the driver.\r\n* All the entities (Police, driver etc.) have their own wallet address acting as a unique identifier.\r\n* A traffic officer can easily query those information with any device.\r\n* Permissions are put into place to restrict access to data to certain parties to protect privacy.\r\n* Traffic officer issues a ticket which is, under the hood, a smart contract.\r\n* This allows seamless interaction with the ticket between interested parties.\r\n* Alleged rule breaker (driver/owner) can easily pay the fines or even decide to fight it in court.\r\n* The ticket can handle these situations and change its state accordingly.\r\n* Similarly, the ticket can also track its due date and incur late fees, and even deploy arrest warrants.\r\n* All of these are achievable for the smart contract while being secure, reliable, transparent and without any human interventions.\r\n\r\n\u003CImageWithCaption\r\n  src={t3}\r\n  caption=\"Password-less login with MetaMask\"\r\n/>\r\n\r\n## Benefits\r\n1. Blockchain technology saves money by minimizing the need for human intervention.\r\n2. It accelerates the payment mechanism immensely compared to the current system without any extra costs.\r\n3. The blockchain system saves energy due to the recent advancements by Proof of Stake.\r\n4. Decentralized database guarantees zero data loss.\r\n\r\n\r\n## Demo\r\nHere is a video of the project in action:\r\n\r\n\u003Cdiv className={\"w-fit mx-auto\"}>\r\n  \u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rQKR1OpysRg?si=xuzyzxmDB152a-wG\"\r\n          title=\"YouTube video player\" frameborder=\"0\"\r\n          allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\r\n          allowfullscreen>\u003C/iframe>\r\n\u003C/div>","src/content/project/traffichain.mdx",[165],"/src/assets/img/project/traffichain/t1.png","75f96448106f0a15","traffichain.mdx"]